# All DataScience

This document consists on the notes about various data science artifacts: what they are, how they are called.

Also, it can link to additional articles / examples to better understand a specific item.

## Glossary

 - **Bias**:

"Bias measures how well a trained (on training dataset) algorithm can generalize on unseen data. High bias (underfitting) meaning the model cannot work well on unseen data." [1](##References)

 - **Convolutional Neural Network (CNN)**:

"We usually use CNNs for work related to images (classification, context extraction, etc). They are very powerful at extracting features from features from features, etc." [1](##References)

"CNNs’ ability to detect features can be used for extracting information about patterns in GS’s stock price movements." [1](##References)

"CNNs work well on spatial data — meaning data points that are closer to each other are more related to each other, than data points spread across. This should hold true for time series data." [1](##References)

 - **GAN**:

Stands for "Generative Adversarial Network".

 - **Policy Optimization**: 

"in policy optimization we learn the action to take from a given state. (if we use methods like Actor/Critic) we also learn the value of being in a given state" [1](##References)

 - **Posterior Probability**:

"A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information. The posterior probability is calculated by updating the prior probability using Bayes' theorem. In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred." [2](##References)

 - **Q-learning**:

"in Q-learning we learn the value of taking an action from a given state. Q-value is the expected return after taking the action." [1](##References)

 - **Reinforcement Learning (RL)**:

"One crucial aspect of building a RL algorithm is accurately setting the reward. It has to capture all aspects of the environment and the agent’s interaction with the environment." [1](##References)

 - **Reinforcement Learning (RL) for Parameter Optimization"**:

"The purpose of the whole reinforcement learning part of this notebook is more research oriented. We will explore different RL approaches using the GAN as an environment. There are many ways in which we can successfully perform hyperparameter optimization on our deep learning models without using RL. But… why not." [1](##References)

 - **Variance**:

"Variance measures the sensitivity of the model to changes in the dataset. High variance is the overfitting." [1](##References)

## References

[1] Using the latest advancements in deep learning to predict stock price movements. https://towardsdatascience.com/aifortrading-2edd6fac689d

[2] Posterior Probability Definition. https://www.investopedia.com/terms/p/posterior-probability.asp#:~:text=A%20posterior%20probability%2C%20in%20Bayesian,taking%20into%20consideration%20new%20information.&text=In%20statistical%20terms%2C%20the%20posterior,that%20event%20B%20has%20occurred.
